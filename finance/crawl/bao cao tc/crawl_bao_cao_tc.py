from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import pandas as pd
import time

# C·∫•u h√¨nh Selenium
chrome_options = Options()
# chrome_options.add_argument("--headless")  # Ch·∫°y kh√¥ng giao di·ªán ƒë·ªÉ nhanh h∆°n
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")

# ƒê∆∞·ªùng d·∫´n ƒë·∫øn ChromeDriver (C·∫≠p nh·∫≠t ƒë√∫ng v·ªã tr√≠ c·ªßa b·∫°n)
service = Service("/Users/nguyentiendang0106/Downloads/chromedriver-mac-arm64/chromedriver")
driver = webdriver.Chrome(service=service, options=chrome_options)

# URL danh s√°ch m√£ ch·ª©ng kho√°n tr√™n Vietstock
url = "https://vietstock.vn/tai-chinh.htm"
driver.get(url)
time.sleep(5)  # Ch·ªù trang t·∫£i

# L·∫•y HTML sau khi trang ƒë√£ t·∫£i
soup = BeautifulSoup(driver.page_source, 'html.parser')

# T√¨m t·∫•t c·∫£ m√£ ch·ª©ng kho√°n trong class ch·ª©a t√™n m√£
stocks = soup.find_all("span", class_="name-index")

# L∆∞u danh s√°ch m√£ ch·ª©ng kho√°n v√† link
stock_list = []
for stock in stocks:
    link = stock.find("a")["href"] if stock.find("a") else None
    ticker = stock.find("a").text.strip() if stock.find("a") else None
    if ticker and link:
        stock_list.append((ticker, link))

# ƒê√≥ng tr√¨nh duy·ªát sau khi l·∫•y d·ªØ li·ªáu
driver.quit()

# L∆∞u danh s√°ch m√£ ch·ª©ng kho√°n v√†o DataFrame
df_stocks = pd.DataFrame(stock_list, columns=["Ticker", "URL"])
df_stocks.to_csv("danh_sach_ma_chung_khoan.csv", index=False)
print(f"‚úÖ ƒê√£ l·∫•y {len(df_stocks)} m√£ ch·ª©ng kho√°n!")

# Kh·ªüi ƒë·ªông l·∫°i Selenium
driver = webdriver.Chrome(service=service, options=chrome_options)

# ƒê·ªçc danh s√°ch m√£ ch·ª©ng kho√°n t·ª´ file CSV (ƒë√£ crawl ·ªü b∆∞·ªõc tr∆∞·ªõc)
df_stocks = pd.read_csv("danh_sach_ma_chung_khoan.csv")

# T·∫°o Dictionary ƒë·ªÉ l∆∞u d·ªØ li·ªáu t·ª´ng b·∫£ng
financial_reports = {
    "KQKD": [],  # K·∫øt qu·∫£ kinh doanh
    "CƒêKT": [],  # C√¢n ƒë·ªëi k·∫ø to√°n
    "CSKD": []   # Ch·ªâ s·ªë t√†i ch√≠nh
}

# H√†m l·∫•y d·ªØ li·ªáu t·ª´ b·∫£ng
def extract_table_data(table):
    data = []
    if table:
        rows = table.find_all("tr")
        for row in rows:
            cols = row.find_all("td")
            cols_text = [col.get_text(strip=True) for col in cols]
            if len(cols_text) > 1:
                data.append(cols_text)
    return data

# L·∫∑p qua t·ª´ng m√£ ch·ª©ng kho√°n
for index, row in df_stocks.iterrows():
    ticker = row["Ticker"]
    url = row["URL"]

    print(f"üîç ƒêang l·∫•y d·ªØ li·ªáu b√°o c√°o t√†i ch√≠nh c·ªßa {ticker}...")

    driver.get(url)
    time.sleep(5)  # Ch·ªù trang t·∫£i

    # L·∫•y HTML trang web
    soup = BeautifulSoup(driver.page_source, 'html.parser')

    # L·∫•y 3 b·∫£ng b√°o c√°o t√†i ch√≠nh
    tables = {
        "KQKD": soup.find("table", id="table-0"),  # K·∫øt qu·∫£ kinh doanh
        "CƒêKT": soup.find("table", id="table-1"),  # C√¢n ƒë·ªëi k·∫ø to√°n
        "CSKD": soup.find("table", id="table-2")   # Ch·ªâ s·ªë t√†i ch√≠nh
    }

    # L∆∞u d·ªØ li·ªáu c·ªßa t·ª´ng b·∫£ng
    for key, table in tables.items():
        data = extract_table_data(table)
        for row in data:
            financial_reports[key].append([ticker] + row)

# ƒê√≥ng tr√¨nh duy·ªát sau khi l·∫•y d·ªØ li·ªáu
driver.quit()

# Chuy·ªÉn d·ªØ li·ªáu th√†nh DataFrame
df_kqkd = pd.DataFrame(financial_reports["KQKD"])
df_cdkt = pd.DataFrame(financial_reports["CƒêKT"])
df_cskd = pd.DataFrame(financial_reports["CSKD"])

# L∆∞u v√†o file Excel v·ªõi 3 sheet
with pd.ExcelWriter("bao_cao_tai_chinh.xlsx") as writer:
    df_kqkd.to_excel(writer, sheet_name="Ket_Qua_Kinh_Doanh", index=False)
    df_cdkt.to_excel(writer, sheet_name="Can_Doi_Ke_Toan", index=False)
    df_cskd.to_excel(writer, sheet_name="Chi_So_Tai_Chinh", index=False)

print("‚úÖ ƒê√£ l∆∞u b√°o c√°o t√†i ch√≠nh v√†o 'bao_cao_tai_chinh.xlsx' v·ªõi 3 sheet.")